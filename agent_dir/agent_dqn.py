import os
import random
import copy
import numpy as np
import torch
from pathlib import Path
from tensorboardX import SummaryWriter
from torch import nn, optim
from agent_dir.agent import Agent
from collections import deque
import matplotlib.pyplot as plt


class QNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=3):
        super(QNetwork, self).__init__()
        self.num_layers = num_layers
        
        # æ„å»ºå¤šå±‚ç½‘ç»œ - ä¿®å¤BatchNormé—®é¢˜
        layers = []
        current_size = input_size
        
        for i in range(num_layers):
            layers.append(nn.Linear(current_size, hidden_size))
            layers.append(nn.ReLU())
            # ä½¿ç”¨LayerNormæ›¿ä»£BatchNormï¼Œé¿å…å°æ‰¹æ¬¡é—®é¢˜
            layers.append(nn.LayerNorm(hidden_size))
            layers.append(nn.Dropout(0.1))
            current_size = hidden_size
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(hidden_size, output_size))
        
        self.network = nn.Sequential(*layers)

    def forward(self, inputs):
        return self.network(inputs)


class ReplayBuffer:
    def __init__(self, buffer_size):
        self.buffer = deque(maxlen=buffer_size)

    def __len__(self):
        return len(self.buffer)

    def push(self, *transition):
        self.buffer.append(transition)

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def clean(self):
        self.buffer.clear()


class AgentDQN(Agent):
    def __init__(self, env, args):
        """
        Initialize every things you need here.
        For example: building your model
        """
        super(AgentDQN, self).__init__(env)
        
        # ç¯å¢ƒå‚æ•°
        self.env = env
        self.state_size = env.observation_space.shape[0]
        self.action_size = env.action_space.n
        
        # è¶…å‚æ•° - GPUä¼˜åŒ–ç‰ˆæœ¬
        self.lr = args.lr
        self.gamma = args.gamma
        self.epsilon = 1.0
        self.epsilon_min = 0.05
        self.epsilon_decay = getattr(args, 'epsilon_decay', 0.9995)
        self.batch_size = getattr(args, 'batch_size', 64)  # ç¡®ä¿æœ€å°æ‰¹æ¬¡å¤§å°
        self.memory_size = getattr(args, 'memory_size', 100000)
        self.update_target_freq = getattr(args, 'update_target_freq', 500)
        self.hidden_size = args.hidden_size
        self.n_frames = args.n_frames
        self.train_freq = getattr(args, 'train_freq', 1)
        self.num_layers = getattr(args, 'num_layers', 3)
        
        # ç¡®ä¿æ‰¹æ¬¡å¤§å°è‡³å°‘ä¸º2ï¼Œé¿å…BatchNormé—®é¢˜
        self.batch_size = max(self.batch_size, 2)
        
        # å­¦ä¹ ç‡è¡°å‡å‚æ•° - ä»å‘½ä»¤è¡Œå‚æ•°è·å–
        self.lr_decay = args.lr_decay
        self.lr_min = args.lr_min
        self.lr_patience = args.lr_patience
        self.lr_factor = args.lr_factor
        self.initial_lr = self.lr
        self.best_avg_reward = -float('inf')
        self.lr_stagnant_count = 0
        
        # æ”¶æ•›åˆ¤æ–­å‚æ•° - ä»å‘½ä»¤è¡Œå‚æ•°è·å–
        self.convergence_window = args.convergence_window
        self.convergence_threshold = args.convergence_threshold
        self.convergence_check_interval = args.convergence_check_interval
        
        # æ¢¯åº¦è£å‰ª
        self.grad_clip = args.grad_norm_clip
        
        # è®¾å¤‡
        self.device = torch.device("cuda" if args.use_cuda and torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # GPUä¼˜åŒ–è®¾ç½®
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–cuDNNæ€§èƒ½
            torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§ç®—æ³•ä»¥æé«˜æ€§èƒ½
        
        # ç½‘ç»œ - æ”¯æŒå¤šå±‚
        self.q_network = QNetwork(self.state_size, self.hidden_size, self.action_size, self.num_layers).to(self.device)
        self.target_network = QNetwork(self.state_size, self.hidden_size, self.action_size, self.num_layers).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.lr, weight_decay=1e-5)
        
        # ä½¿ç”¨æ›´aggressiveçš„å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=self.lr_decay)
        
        # ç»éªŒå›æ”¾
        self.memory = ReplayBuffer(self.memory_size)
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # è®­ç»ƒç»Ÿè®¡
        self.frame_count = 0
        self.episode_count = 0
        
        # TensorBoard
        self.writer = SummaryWriter('logs/dqn')
        
        # æ‰¹å¤„ç†ä¼˜åŒ–
        self.state_buffer = []
        self.action_buffer = []
        self.reward_buffer = []
        self.next_state_buffer = []
        self.done_buffer = []
        
        # ç»˜å›¾ç›¸å…³
        self.episode_rewards = []
        self.avg_rewards = []
        self.episodes = []
        
        # åˆ›å»ºå›¾å½¢ç›®å½•
        os.makedirs('plots', exist_ok=True)
        
        # è®¾ç½®matplotlibä¸ºéäº¤äº’æ¨¡å¼
        plt.ioff()
    
    def init_game_setting(self):
        """
        Testing function will call this function at the begining of new game
        Put anything you want to initialize if necessary
        """
        self.epsilon = 0.01  # æµ‹è¯•æ—¶ä½¿ç”¨è¾ƒå°çš„epsilon

    def rebuild_network(self, hidden_size=None, num_layers=None):
        """é‡æ–°æ„å»ºç½‘ç»œä»¥æ”¯æŒä¸åŒçš„æ¶æ„"""
        if hidden_size:
            self.hidden_size = hidden_size
        if num_layers:
            self.num_layers = num_layers
        
        # é‡æ–°åˆ›å»ºç½‘ç»œ
        self.q_network = QNetwork(self.state_size, self.hidden_size, self.action_size, self.num_layers).to(self.device)
        self.target_network = QNetwork(self.state_size, self.hidden_size, self.action_size, self.num_layers).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.lr, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=self.lr_decay)
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.target_network.load_state_dict(self.q_network.state_dict())

    def train(self):
        """
        Implement your training algorithm here - GPUä¼˜åŒ–ç‰ˆæœ¬
        """
        if len(self.memory) < self.batch_size:
            return
        
        # ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒä»¥æé«˜GPUåˆ©ç”¨ç‡
        effective_batch_size = min(self.batch_size, len(self.memory))
        
        # ç¡®ä¿æ‰¹æ¬¡å¤§å°è‡³å°‘ä¸º2
        if effective_batch_size < 2:
            return
        
        # ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·
        batch = self.memory.sample(effective_batch_size)
        
        # é¢„åˆ†é…tensorä»¥æé«˜å†…å­˜æ•ˆç‡
        states = torch.empty((effective_batch_size, self.state_size), dtype=torch.float32, device=self.device)
        actions = torch.empty(effective_batch_size, dtype=torch.long, device=self.device)
        rewards = torch.empty(effective_batch_size, dtype=torch.float32, device=self.device)
        next_states = torch.empty((effective_batch_size, self.state_size), dtype=torch.float32, device=self.device)
        dones = torch.empty(effective_batch_size, dtype=torch.bool, device=self.device)
        
        # å¡«å……æ•°æ®
        for i, (state, action, reward, next_state, done) in enumerate(batch):
            states[i] = torch.from_numpy(np.array(state))
            actions[i] = action
            rewards[i] = reward
            next_states[i] = torch.from_numpy(np.array(next_state))
            dones[i] = done
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # Double DQN: ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°Qå€¼
        with torch.no_grad():
            next_actions = self.q_network(next_states).max(1)[1].unsqueeze(1)
            next_q_values = self.target_network(next_states).gather(1, next_actions).squeeze(1)
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), self.grad_clip)
        
        self.optimizer.step()
        
        # æ›´æ–°epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # å­¦ä¹ ç‡è¡°å‡
        if self.frame_count % 1000 == 0:
            current_lr = self.optimizer.param_groups[0]['lr']
            if current_lr > self.lr_min:
                self.scheduler.step()

    def check_convergence(self, reward_history):
        """
        æ£€æŸ¥æ˜¯å¦æ”¶æ•›ï¼šåˆ¤æ–­æœ€è¿‘Nä¸ªå›åˆçš„å¹³å‡å¥–åŠ±æ˜¯å¦å¤§äºé˜ˆå€¼
        """
        if len(reward_history) < self.convergence_window:
            return False, 0
            
        # è·å–æœ€è¿‘Nä¸ªå›åˆçš„å¥–åŠ±
        recent_rewards = list(reward_history)[-self.convergence_window:]
        mean_reward = np.mean(recent_rewards)
        
        # åˆ¤æ–­æ˜¯å¦æ”¶æ•›ï¼šå¹³å‡å¥–åŠ±å¤§äºé˜ˆå€¼
        is_converged = mean_reward >= self.convergence_threshold
        
        return is_converged, mean_reward

    def make_action(self, observation, test=True):
        """
        Return predicted action of your agent
        Input:observation
        Return:action
        """
        if not test and random.random() < self.epsilon:
            return random.randrange(self.action_size)
        
        # ç¡®ä¿è¾“å…¥æ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if isinstance(observation, np.ndarray):
            state = torch.FloatTensor(observation).unsqueeze(0).to(self.device)
        else:
            state = torch.FloatTensor([observation]).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            q_values = self.q_network(state)
        return q_values.max(1)[1].item()

    def plot_rewards(self, save_path='plots/reward_curve.png'):
        """
        ç»˜åˆ¶rewardæ›²çº¿å›¾
        """
        plt.figure(figsize=(12, 6))
        
        # ç»˜åˆ¶æ¯å›åˆå¥–åŠ±
        plt.plot(self.episodes, self.episode_rewards, 'b-', alpha=0.6, label='Episode Reward')
        plt.axhline(y=self.convergence_threshold, color='g', linestyle='--', label=f'Target ({self.convergence_threshold})')
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.title('Episode Rewards')
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Reward curve saved to {save_path}")

    def run(self):
        """
        Implement the interaction between agent and environment here - GPUä¼˜åŒ–ç‰ˆæœ¬
        """
        reward_history = deque(maxlen=100)
        all_rewards = []
        
        print(f"å¼€å§‹DQNè®­ç»ƒ... (GPUä¼˜åŒ–ç‰ˆæœ¬)")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}, ç½‘ç»œå±‚æ•°: {self.num_layers}, éšè—å±‚å¤§å°: {self.hidden_size}")
        print(f"æ”¶æ•›æ¡ä»¶ï¼šæœ€è¿‘{self.convergence_window}å›åˆå¹³å‡å¥–åŠ±â‰¥{self.convergence_threshold}")
        print("-" * 70)
        
        while self.frame_count < self.n_frames:
            state = self.env.reset()[0]
            episode_reward = 0
            done = False
            
            while not done:
                action = self.make_action(state, test=False)
                next_state, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                # å­˜å‚¨ç»éªŒ
                self.memory.push(state, action, reward, next_state, done)
                
                state = next_state
                episode_reward += reward
                self.frame_count += 1
                
                # æ›´é¢‘ç¹çš„è®­ç»ƒä»¥æé«˜GPUåˆ©ç”¨ç‡
                if self.frame_count % self.train_freq == 0 and len(self.memory) >= self.batch_size:
                    self.train()
                
                # æ›´æ–°ç›®æ ‡ç½‘ç»œ
                if self.frame_count % self.update_target_freq == 0:
                    self.target_network.load_state_dict(self.q_network.state_dict())
                
                if done:
                    break
            
            reward_history.append(episode_reward)
            all_rewards.append(episode_reward)
            self.episode_count += 1
            
            # è®°å½•å¥–åŠ±æ•°æ®
            self.episode_rewards.append(episode_reward)
            self.episodes.append(self.episode_count)
            avg_reward = np.mean(reward_history)
            self.avg_rewards.append(avg_reward)
            
            # è®°å½•æ—¥å¿—
            if self.episode_count % 10 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"Episode: {self.episode_count}, Frames: {self.frame_count}, "
                      f"Avg Reward: {avg_reward:.2f}, Epsilon: {self.epsilon:.3f}, LR: {current_lr:.6f}")
                
                self.writer.add_scalar('Reward/Episode', episode_reward, self.episode_count)
                self.writer.add_scalar('Reward/Average', avg_reward, self.episode_count)
                self.writer.add_scalar('Epsilon', self.epsilon, self.episode_count)
                self.writer.add_scalar('Learning_Rate', current_lr, self.episode_count)
                
                # æ¯50å›åˆä¿å­˜ä¸€æ¬¡å›¾
                if self.episode_count % 50 == 0:
                    self.plot_rewards()
                
                # æ£€æŸ¥æ”¶æ•›
                if self.episode_count % self.convergence_check_interval == 0:
                    is_converged, mean_reward = self.check_convergence(all_rewards)
                    
                    if is_converged:
                        print("-" * 70)
                        print(f"ğŸ‰ ç®—æ³•æ”¶æ•›ï¼åœ¨ç¬¬{self.episode_count}å›åˆè¾¾åˆ°ç¨³å®šçŠ¶æ€")
                        print(f"ğŸ“Š æœ€è¿‘{self.convergence_window}å›åˆå¹³å‡å¥–åŠ±: {mean_reward:.2f}")
                        print(f"ğŸ¯ ä½¿ç”¨æ ·æœ¬æ•°: {self.frame_count}")
                        self.plot_rewards('plots/final_reward_curve.png')
                        break
                    elif len(all_rewards) >= self.convergence_window:
                        # è¾“å‡ºæ”¶æ•›çŠ¶æ€ä¿¡æ¯
                        print(f"   æ”¶æ•›æ£€æŸ¥: å¹³å‡={mean_reward:.2f}")
                
                # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
                if self.episode_count % 50 == 0:  # æ¯50å›åˆæ£€æŸ¥ä¸€æ¬¡æ€§èƒ½
                    current_lr = self.optimizer.param_groups[0]['lr']
                    if avg_reward > self.best_avg_reward:
                        self.best_avg_reward = avg_reward
                        self.lr_stagnant_count = 0
                    else:
                        self.lr_stagnant_count += 1
                    
                    # å¦‚æœé•¿æ—¶é—´æ²¡æœ‰æ”¹å–„ä¸”å­¦ä¹ ç‡è¿˜ä¸æ˜¯æœ€å°å€¼ï¼Œæ‰‹åŠ¨é™ä½å­¦ä¹ ç‡
                    if self.lr_stagnant_count >= 10 and current_lr > self.lr_min:
                        new_lr = max(current_lr * self.lr_factor, self.lr_min)
                        for param_group in self.optimizer.param_groups:
                            param_group['lr'] = new_lr
                        print(f"   æ€§èƒ½åœæ»{self.lr_stagnant_count}æ¬¡ï¼Œé™ä½å­¦ä¹ ç‡è‡³: {new_lr:.6f}")
                        self.lr_stagnant_count = 0
        else:
            # è¾¾åˆ°æœ€å¤§å¸§æ•°ä½†æœªæ”¶æ•›
            print("-" * 70)
            print(f"è®­ç»ƒç»“æŸï¼šè¾¾åˆ°æœ€å¤§å¸§æ•°{self.n_frames}")
            is_converged, mean_reward = self.check_convergence(all_rewards)
            if is_converged:
                print(f"âœ… å·²æ”¶æ•›: å¹³å‡å¥–åŠ±{mean_reward:.2f}")
            else:
                print(f"âš ï¸  æœªæ”¶æ•›: å¹³å‡å¥–åŠ±{mean_reward:.2f}")
                print(f"ğŸ’¡ å»ºè®®ï¼šå¢åŠ è®­ç»ƒæ—¶é—´æˆ–è°ƒæ•´è¶…å‚æ•°ä»¥æé«˜ç¨³å®šæ€§")
        
        # è®­ç»ƒç»“æŸåç»˜åˆ¶æœ€ç»ˆæ›²çº¿
        self.plot_rewards('plots/final_reward_curve.png')
        self.writer.close()
        print(f"Training completed. Final average reward: {np.mean(reward_history):.2f}")
